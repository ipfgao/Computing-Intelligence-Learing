{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 复习上课内容以及复现课程代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本部分，你需要复习上课内容和课程代码后，自己复现课程代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, inputs=[]):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "\n",
    "        for n in self.inputs:\n",
    "            n.outputs.append(self)\n",
    "            # set 'self' node as inbound_nodes's outbound_nodes\n",
    "\n",
    "        self.value = None\n",
    "\n",
    "        self.gradients = {}\n",
    "        # keys are the inputs to this node, and their\n",
    "        # values are the partials of this node with \n",
    "        # respect to that input.\n",
    "        # \\partial{node}{input_i}\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        '''\n",
    "        Forward propagation. \n",
    "        Compute the output value vased on 'inbound_nodes' and store the \n",
    "        result in self.value\n",
    "        '''\n",
    "\n",
    "        raise NotImplemented\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        raise NotImplemented\n",
    "        \n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        An Input node has no inbound nodes.\n",
    "        So no need to pass anything to the Node instantiator.\n",
    "        '''\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self, value=None):\n",
    "        '''\n",
    "        Only input node is the node where the value may be passed\n",
    "        as an argument to forward().\n",
    "        All other node implementations should get the value of the \n",
    "        previous node from self.inbound_nodes\n",
    "        \n",
    "        Example: \n",
    "        val0: self.inbound_nodes[0].value\n",
    "        '''\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "            ## It's is input node, when need to forward, this node initiate self's value.\n",
    "\n",
    "        # Input subclass just holds a value, such as a data feature or a model parameter(weight/bias)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.gradients = {self:0}\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost * 1\n",
    "            \n",
    "        \n",
    "        # input N --> N1, N2\n",
    "        # \\partial L / \\partial N \n",
    "        # ==> \\partial L / \\partial N1 * \\ partial N1 / \\partial N\n",
    "\n",
    "\n",
    "class Add(Node):\n",
    "    def __init__(self, *nodes):\n",
    "        Node.__init__(self, nodes)\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = sum(map(lambda n: n.value, self.inputs))\n",
    "        ## when execute forward, this node caculate value as defined.\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        Node.__init__(self, [nodes, weights, bias])\n",
    "\n",
    "    def forward(self):\n",
    "        inputs = self.inputs[0].value\n",
    "        weights = self.inputs[1].value\n",
    "        bias = self.inputs[2].value\n",
    "\n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "        \n",
    "    def backward(self):\n",
    "\n",
    "        # initial a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            # Get the partial of the cost w.r.t this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "\n",
    "            self.gradients[self.inputs[0]] = np.dot(grad_cost, self.inputs[1].value.T)\n",
    "            self.gradients[self.inputs[1]] = np.dot(self.inputs[0].value.T, grad_cost)\n",
    "            self.gradients[self.inputs[2]] = np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "        # WX + B / W ==> X\n",
    "        # WX + B / X ==> W\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1 + np.exp(-1 * x))\n",
    "\n",
    "    def forward(self):\n",
    "        self.x = self.inputs[0].value\n",
    "        self.value = self._sigmoid(self.x)\n",
    "\n",
    "    def backward(self):\n",
    "        self.partial = self._sigmoid(self.x) * (1 - self._sigmoid(self.x))\n",
    "        \n",
    "        # y = 1 / (1 + e^-x)\n",
    "        # y' = 1 / (1 + e^-x) (1 - 1 / (1 + e^-x))\n",
    "        \n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]  # Get the partial of the cost with respect to this node.\n",
    "\n",
    "            self.gradients[self.inputs[0]] = grad_cost * self.partial\n",
    "            # use * to keep all the dimension same!.\n",
    "\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        y = self.inputs[0].value.reshape(-1, 1)\n",
    "        a = self.inputs[1].value.reshape(-1, 1)\n",
    "        assert(y.shape == a.shape)\n",
    "\n",
    "        self.m = self.inputs[0].value.shape[0]\n",
    "        self.diff = y - a\n",
    "\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        self.gradients[self.inputs[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inputs[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def forward_and_backward(outputnode, graph):\n",
    "    # execute all the forward method of sorted_nodes.\n",
    "\n",
    "    ## In practice, it's common to feed in mutiple data example in each forward pass rather than just 1. Because the examples can be processed in parallel. The number of examples is called batch size.\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "        ## each node execute forward, get self.value based on the topological sort result.\n",
    "\n",
    "    for n in  graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "    #return outputnode.value\n",
    "\n",
    "###   v -->  a -->  C\n",
    "##    b --> C\n",
    "##    b --> v -- a --> C\n",
    "##    v --> v ---> a -- > C\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outputs:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "            ## if n is Input Node, set n'value as \n",
    "            ## feed_dict[n]\n",
    "            ## else, n's value is caculate as its\n",
    "            ## inbounds\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    # there are so many other update / optimization methods\n",
    "    # such as Adam, Mom, \n",
    "    for t in trainables:\n",
    "        t.value += -1 * learning_rate * t.gradients[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What does a neuron compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.什么是神经元计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：输入一些信号，放进一个“神经元”（按权重累加输入信号，然后用激活函数计算）里进行处理，然后输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Why we use non-linear activation funcitons in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.我们在神经网络中为什么使用非线性激活函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：如果用线性激活函数，那么多层运算后依然是线性的，体现不出来神经网络多层计算的意义，不如直接线性回归来的容易。\n",
    "   \n",
    "   事物发展的规律大多数都是非线性发展的，简单的线性回归是满足不了实际需求的。但是我们又不知道该用什么样的函数来构建模型。\n",
    "   神经网络就是为了构建出一个非线性的函数，但又想用线性回归来做。而激活函数（非线性变换函数）就是为了把线性的变成非线性。通过激活函数结合线性函数的多级展开式的形式构建函数模型。   \n",
    "   如下是一个 3 2 1 的三层神经网络构建的函数模型："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma (w_{1,1}^{[3]}\\sigma (w_{1,1}^{[2]}\\sigma(w_{1,1}^{[1]}x_1+w_{1,2}^{[1]}x_2+w_{1,3}^{[1]}x_3)+w_{1,2}^{[2]}\\sigma(w_{2,1}^{[1]}x_1+w_{2,2}^{[1]}x_2+w_{2,3}^{[1]}x_3)+w_{1,3}^{[2]}\\sigma(w_{3,1}^{[1]}x_1+w_{3,2}^{[1]}x_2+w_{3,3}^{[1]}x_3)) + w_{1,2}^{[3]}\\sigma (w_{2,1}^{[2]}\\sigma(w_{1,1}^{[1]}x_1+w_{1,2}^{[1]}x_2+w_{1,3}^{[1]}x_3)+w_{2,2}^{[2]}\\sigma(w_{2,1}^{[1]}x_1+w_{2,2}^{[1]}x_2+w_{2,3}^{[1]}x_3)+w_{2,3}^{[2]}\\sigma(w_{3,1}^{[1]}x_1+w_{3,2}^{[1]}x_2+w_{3,3}^{[1]}x_3))) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the 'Logistic Loss' ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.逻辑回归的损失函数是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：$L=-ylog\\hat y+(y-1)log(1-  \\hat y)$  \n",
    "变化规律相当于$\\hat y-2y\\hat y + y $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?\n",
    "\n",
    "A. ReLU    \n",
    "B. Leaky ReLU    \n",
    "C. sigmoid    \n",
    "D. tanh  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.假设你正在构建一个二元分类器来识别图片中是不是有猫，你会在输出层使用什么样的激活函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A.线性整流函数(ReLU)  \n",
    "B.带泄露线性整流函数函数(Leaky ReLU)  \n",
    "C.sigmoid函数  \n",
    "D.双曲正切函数(tanh)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：C，二分类的输出是概率，需要把结果压缩在0~1之间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Why we don't use zero initialization for all parameters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.我们为什么不用0初始化所有参数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：都初始化为0，各个“神经元”处理出来的都是一样的数据，多个神经元就没有意义了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you implement the softmax function using python ?   \n",
    "#### 6. 你能用python来实现softmax函数吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：\n",
    "\n",
    "$softmax：y_j=\\frac {e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_j$：隐藏层的输出，经输入加权后的累加，j为输出类的序号  \n",
    "$K$：输出的总类数，$j$的范围是1~$K$   \n",
    "$\\sum_{k=1}^K e^{z_k}$:从$k=1$开始至$K$，累加$e^{z_k}$\n",
    "\n",
    "设softmax之前由$z_j$组成的数组为X，则："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def softmax(X):\n",
    "    exps=np.exp(X)\n",
    "    return exps/np.sum(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07629314 0.56373431 0.07629314 0.07629314 0.20738626]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=[1,3,1,1,2]\n",
    "P=softmax(X)\n",
    "print(P)\n",
    "np.sum(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，经过softmax处理后，每个类输出的概率均在0~1之间，而总概率为1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss函数$L=\\sum_{i=1}^Ky_ilog\\hat y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this practical part, you will build a simple digits recognizer（数字识别器） to check if the digit in the image is larger than 5. This assignmnet will guide you step by step to finish your first small project in this course ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Packages  \n",
    "sklearn is a famous package for machine learning.   \n",
    "matplotlib is a common package for vasualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Overvie of the dataset  \n",
    "    - a training set has m_train images labeled as 0 if the digit < 5 or 1 if the digit >= 5\n",
    "    - a test set contains m_test images labels as if the digit < 5 or 1 if the digit >= 5\n",
    "    - eah image if of shape (num_px, num_px ). Thus, each image is square(height=num_px and  width = num_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data \n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vilizating the data\n",
    "for i in range(1,11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(digits.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)\n",
    "    plt.text(3,10,str(digits.target[i-1]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training set and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformulate the label. \n",
    "# If the digit is smaller than 5, the label is 0.\n",
    "# If the digit is larger than 5, the label is 1.\n",
    "\n",
    "y_train[y_train < 5 ] = 0\n",
    "y_train[y_train >= 5] = 1\n",
    "y_test[y_test < 5] = 0\n",
    "y_test[y_test >= 5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64)\n",
      "(450, 64)\n",
      "(1347,)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Architecture of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ipfgao/Computing-Intelligence/ipfgao/assignment4\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathematical expression of the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one example $x^{(i)}$:   \n",
    " $$ z^{(i)} = w^T * x^{(i)} +b $$   \n",
    " $$ y^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$   \n",
    " $$L(a^{(i)},y^{(i)}) = -y^{(i)} log(a^{(i)})-(1-y^{(i)})log(1-a^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total cost over all training examples:\n",
    "$$ J = \\frac{1}{m}\\sum_{i=1}^{m}L(a^{(i)},y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Building the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1- Activation function    \n",
    "###### Exercise:\n",
    "Finish the sigmoid funciton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Compute the sigmoid of z\n",
    "    Arguments: z -- a scalar or numpy array of any size.\n",
    "    \n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    '''\n",
    "    s =1/(1+ np.exp(-z))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0,2]) = [0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "# Test your code \n",
    "# The result should be [0.5 0.88079708]\n",
    "print(\"sigmoid([0,2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1-Initializaing parameters\n",
    "###### Exercise:\n",
    "Finishe the initialize_parameters function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random innitialize the parameters\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    '''\n",
    "    Argument: dim -- size of the w vector  #总共有多少个w\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim,1) #shape(dim,1),表示dim行，1列的向量\n",
    "    b -- initializaed scalar\n",
    "    '''\n",
    "    import random\n",
    "    w = np.random.randn(dim,1)               #生产dim行，1列的随机数,\n",
    "    b = random.randint(-1,1)                  #随机数b\n",
    "    \n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b,float) or isinstance(b,int))\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3-Forward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some mathematical expressions\n",
    "Forward Propagation:   \n",
    ". X    \n",
    ". A = $\\sigma(w^T*X+b) = (a^{(1)},a^{(2)},...,a^{(m)})$   \n",
    ". J = $-\\frac{1}{m} \\sum_{i=1}^{m}(y^{(i)}loga^{(i)}+(1-y^{(i)})log(1-a^{(i)}))$       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some derivative: \n",
    "$$\\frac{\\partial{J}}{\\partial{w}} = \\frac{1}{m}X*(A-Y)^T$$   \n",
    "$$\\frac{\\partial{J}}{\\partial{b}} = \\frac{1}{m}\\sum_{i=1}^m(a^{(i)}-y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Finish the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w,b,X,Y):\n",
    "    '''\n",
    "    Implement the cost function and its gradient for the propagation\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data，类型是np.array\n",
    "    Y - ground truth\n",
    "    '''\n",
    "    m = X.shape[1]                       #列数，样本总数\n",
    "    A = sigmoid(np.dot(w.T,X)+b)        #w.T,将w转置\n",
    "    cost = -1/m*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))\n",
    "    \n",
    "    dw = 1/m*np.dot(X,(A-Y).T)\n",
    "    db = 1/m*np.sum(A-Y)\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw':dw,\n",
    "             'db':db}\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 -Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Minimizing the cost function using gradient descent.   \n",
    "$$\\theta = \\theta - \\alpha*d\\theta$$ where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    '''\n",
    "    This function optimize w and b by running a gradient descen algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params - dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = propagate(w,b,X,Y)\n",
    "        \n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        w = w-learning_rate*dw\n",
    "        b = b-learning_rate*db\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\":w,\n",
    "              \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise\n",
    "The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function.    \n",
    "Two steps to finish this task:   \n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T*X+b)$   \n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias \n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    #for i in range(A.shape[i]):\n",
    "    #    None \n",
    "    Y_prediction=(A>0.5)+0  \n",
    "    assert(Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5- Merge all functions into a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations !! You have finished all the necessary components for constructing a model. Now, Let's take the challenge to merge all the implemented function into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "    #初始话参数\n",
    "    w=initialize_parameters(X_train.shape[1])[0]\n",
    "    b=initialize_parameters(X_train.shape[1])[1]\n",
    "    \n",
    "    # \n",
    "    D=optimize(w,b,X_train.T,Y_train.T,num_iterations,learning_rate,print_cost)\n",
    "    \n",
    "    Y_prediction=predict(w, b, X_train.T)\n",
    "    Y_accuracy=(Y_train-Y_prediction).tolist()[0]#全是0效果才好\n",
    "    traing_accuracy=Y_accuracy.count(0)/len(Y_accuracy)\n",
    "    \n",
    "    Y_prediction=predict(w, b, X_test.T)\n",
    "    Y_accuracy=(Y_test-Y_prediction).tolist()[0]#全是0效果才好\n",
    "    test_accuracy=Y_accuracy.count(0)/len(Y_accuracy)\n",
    "\n",
    "    d = {\"w\":D[0][\"w\"],\n",
    "         \"b\":D[0][\"b\"],\n",
    "         \"training_accuracy\": traing_accuracy,\n",
    "         \"test_accuracy\":test_accuracy,\n",
    "         \"cost\":D[2][-1]}\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ipfgao/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  del sys.path[0]\n",
      "/home/ipfgao/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'w': array([[ 6.26983846e-01],\n",
       "        [ 1.57725886e-01],\n",
       "        [ 1.78231298e-02],\n",
       "        [ 5.87790975e-03],\n",
       "        [ 5.33494033e-02],\n",
       "        [ 1.27556700e-01],\n",
       "        [ 1.26735390e-01],\n",
       "        [ 3.91039168e-01],\n",
       "        [-1.58997748e+00],\n",
       "        [-1.62179898e-01],\n",
       "        [ 1.34945962e-01],\n",
       "        [ 8.06864811e-02],\n",
       "        [-5.19387772e-02],\n",
       "        [-3.07894152e-02],\n",
       "        [ 1.00369741e-03],\n",
       "        [-2.16586533e-01],\n",
       "        [-6.97552346e-01],\n",
       "        [ 1.23682430e-01],\n",
       "        [ 1.46818003e-01],\n",
       "        [-4.92705964e-02],\n",
       "        [-2.51182737e-01],\n",
       "        [-1.07531969e-01],\n",
       "        [-7.22337269e-02],\n",
       "        [-5.51342344e-01],\n",
       "        [-1.17012750e+00],\n",
       "        [-2.41306151e-01],\n",
       "        [ 5.87278310e-02],\n",
       "        [ 2.27383811e-01],\n",
       "        [ 5.15268579e-02],\n",
       "        [ 1.80983126e-01],\n",
       "        [-1.47582990e-01],\n",
       "        [-7.04240794e-01],\n",
       "        [ 7.90929461e-01],\n",
       "        [-3.06632353e-01],\n",
       "        [-1.44918938e-02],\n",
       "        [ 2.04572826e-01],\n",
       "        [-9.33014787e-02],\n",
       "        [-5.95618589e-03],\n",
       "        [ 8.12323923e-02],\n",
       "        [ 1.56713089e+00],\n",
       "        [ 4.17520613e-01],\n",
       "        [-2.86552016e-02],\n",
       "        [ 9.88394972e-02],\n",
       "        [-5.00351235e-02],\n",
       "        [ 1.92814727e-01],\n",
       "        [ 4.67603094e-02],\n",
       "        [ 6.73476954e-02],\n",
       "        [ 3.45326570e-01],\n",
       "        [-1.08607540e+00],\n",
       "        [ 1.03545353e-02],\n",
       "        [ 1.27494232e-02],\n",
       "        [-7.82142408e-02],\n",
       "        [-3.70528887e-01],\n",
       "        [-1.34359332e-02],\n",
       "        [ 1.04689824e-01],\n",
       "        [-6.18412294e-01],\n",
       "        [-8.26597643e-01],\n",
       "        [-4.20826485e-01],\n",
       "        [ 2.43746646e-02],\n",
       "        [-5.88792334e-02],\n",
       "        [-1.21143556e-01],\n",
       "        [-1.27344278e-01],\n",
       "        [ 6.55454043e-02],\n",
       "        [-1.23871637e+00]]),\n",
       " 'b': -1.0120417381268472,\n",
       " 'training_accuracy': 0.512249443207127,\n",
       " 'test_accuracy': 0.5,\n",
       " 'cost': 0.23739734769662799}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train,y_train,X_test,y_test,10000,0.01,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.选做题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on building your first logistic regression model. It is your time to analyze it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Observe the effect of learning rate on the leraning process.   \n",
    "Hits: plot the learning curve with different learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ipfgao/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  del sys.path[0]\n",
      "/home/ipfgao/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'w': array([[-1.72666802],\n",
       "        [ 1.2072063 ],\n",
       "        [ 0.18104579],\n",
       "        [-0.19768985],\n",
       "        [ 0.56618531],\n",
       "        [ 1.14104408],\n",
       "        [ 1.2881562 ],\n",
       "        [-1.98040702],\n",
       "        [ 1.24866642],\n",
       "        [-1.87061837],\n",
       "        [ 1.24860261],\n",
       "        [ 0.80563429],\n",
       "        [-0.50658115],\n",
       "        [-0.2080818 ],\n",
       "        [-0.12582261],\n",
       "        [ 3.11904679],\n",
       "        [-0.2529963 ],\n",
       "        [ 1.23115076],\n",
       "        [ 1.43538244],\n",
       "        [-0.4221246 ],\n",
       "        [-2.2286298 ],\n",
       "        [-1.00723678],\n",
       "        [-1.07301654],\n",
       "        [-3.07158583],\n",
       "        [-1.62135634],\n",
       "        [-2.37728014],\n",
       "        [ 0.42751407],\n",
       "        [ 2.06115276],\n",
       "        [ 0.44927008],\n",
       "        [ 1.78844523],\n",
       "        [-1.36985136],\n",
       "        [-0.85382021],\n",
       "        [ 0.15895955],\n",
       "        [-2.7150193 ],\n",
       "        [-0.1913384 ],\n",
       "        [ 1.89777231],\n",
       "        [-0.92195932],\n",
       "        [-0.22050877],\n",
       "        [ 0.72930981],\n",
       "        [-0.38267708],\n",
       "        [-1.12726014],\n",
       "        [-0.27421911],\n",
       "        [ 0.95349176],\n",
       "        [-0.49799774],\n",
       "        [ 1.77359878],\n",
       "        [ 0.52691983],\n",
       "        [ 0.75533354],\n",
       "        [ 0.26871973],\n",
       "        [ 0.95803968],\n",
       "        [ 0.18412248],\n",
       "        [ 0.22364455],\n",
       "        [-0.66275918],\n",
       "        [-3.30378684],\n",
       "        [-0.08693704],\n",
       "        [ 0.8151864 ],\n",
       "        [ 0.14273252],\n",
       "        [ 0.68751119],\n",
       "        [-3.71044801],\n",
       "        [ 0.26316641],\n",
       "        [-0.47302392],\n",
       "        [-1.25925732],\n",
       "        [-0.91214433],\n",
       "        [-0.51410412],\n",
       "        [-3.50866218]]),\n",
       " 'b': 0.24605235026012787,\n",
       " 'training_accuracy': 0.4951744617668894,\n",
       " 'test_accuracy': 0.5066666666666667,\n",
       " 'cost': nan}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train,y_train,X_test,y_test,10000,0.1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Observe the effect of iteration_num on the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge ! ! !\n",
    "\n",
    "The original data have images labeled 0,1,2,3,4,5,6,7,8,9. In our logistic model, we only detect if the digit in the image is larger or smaller than 5. Now, Let's go for a more challenging problem. Try to use softmax function to build a model to recognize which digits (0,1,2,3,4,5,6,7,8,9) is in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations ! You have completed assigment 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
